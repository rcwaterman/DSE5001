---
title: "PairProgramming Week 7"
author: "HDS"
date: "2024-09-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

checked 01/03/2025

# Pair Programming,  Module 08,  Permutation, Monte Carlo, Cross-validation and Bayesian Methods, DSE5001

Name: Ryan Waterman
Partner Name:


These are tactics which may be used for

  -hypothesis testing

  -estimating confidence intervals

particularly in situations where we don't have a good statistical model to use

#Permutation Tests

Permutation test are used to test hypotheses of differences between groups, or between variables
We can create permutation versions of t-tests, F-test, R^2 values etc

#Permutation t-test

use the sleep data set as an example

```{R}
data(sleep)
head(sleep)
```

This data set is used as an example for the t-test function in R,  

```{R}
t.test(extra~group, data=sleep)
```
Looking at the results, group 1 has a mean of 0.75,  group 2 has a mean of 2.33, the t value is -1.86

p-value is 0.07939,  which is not significant

So we know the answer, but let's try this using a permutation test, just to see
how permutation tests work

*Stating the Null*

We have an observed difference in the means of |0.75-2.33| = 1.58

If there is no difference between the groups, we could re-assign specimens randomly between
the two groups, literally shuffling the factor identifications of specimens, to 
create a permutation set.   We could calculate the absolute value difference in 
the means of the shuffled data.

If we did this many times, say 10,000 times, then we would have a distribution of 
differences generated by the null hypotheis, and we can compare our observed difference
of 1.58 to the distribution from the permutations, and estimate a probability
that the permutation method could generate a value as large as 1.58 by random shuffling.

This is a *non-parametric* simulation, since we do not estimate any parameters to
create the simulation

```{R}
# library with permute function

library("gtools")
```


```{R}

Nperms=10000

# create an empty vector of permuation differences
pdiff=rep(0,Nperms)

#generate 10,000 permutations

extra=sleep$extra
group=sleep$group

for(i in 1:Nperms)
{
  # r has a built in permuation, we will shuffle the values of extra
  perm_extra=permute(extra)
  diffvalue= abs( mean(perm_extra[group==1])-mean(perm_extra[group==2]))
  pdiff[i]=diffvalue
  
}


#create a histogram of the distribution under the null of the permutation test

hist(pdiff,breaks=20)


# our observed difference in the mean was 1.58, show that on the plot
abline(v=1.58)

```

Just looking at the histogram and the plot showing where a difference of 1.58 is 
on the plot,  it looks like the null hypothesis easily produces differences this large

How many pdiff values are greater than 1.58?

```{R}
sum(pdiff>1.58)
```

We have 744 permutation differences of greater than 1.58 in 10,000 trials
{Note: this is a random process, your answer should differ slightly from the 744
I got}

plus the 1 original value out of 1 trial, we can estimate the permutation 
p as 

(744+1)/(10000+1)

```{R}
(744+1)/(10000+1)
```

The permutation p when I ran this code was 0.0745,   the analytic value from the t-test was 0.0793

You will probably get a slightly different p value due to the random nature of the test

*empirical cummulative distribution*

We can get a empirical estimate of the cummulative distribution as well

```{R}

# create the empirical cdf function
perm_cdf=ecdf(pdiff)


# set up to plot
x=seq(0,4,0.1)

cdfvals=perm_cdf(x)

plot(x,cdfvals)
axis(2,at=c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0))

# our observed difference in the mean was 1.58, show that on the plot
abline(v=1.58)

grid()

```

Again, we can see that the permutation has substantial numbers of results above
1.58.

# Monte Carlo

This approach uses the random number generator in R to simulate an event, with
multiple components

Suppose we have the following.   My commute to work takes 12 minutes under
good traffic conditions,  but...

about 1 day in 30, there seems to be a detour.  If there is a detour add 
a random time of 2 to 5 minutes to the time

There is one bad turn onto route 114,   if there are cars in front of me, add
a random normal(mean= 0.75 minute,sd=1/4 minute) time per car in front of me

It seems like at that intersection, the number of cars in front of me is a poison 
distribution with a mean of about 2

Given all this,  what is the distribution of my drive time to work?  What is 
the mean?  the std deviation and the 95% upper bound

We will create nsim=10,000 simulations of this to generate a distribution

This is a *parametric simulation* because we are estimating the parameters used
in the simulation

```{R}

Nsims=10000

# create an empty vector of permuation differences
tsim=rep(0,Nperms)


# ideal drive time without issues
tmin=12

#generate 10,000 simulation


for(i in 1:Nsims)
{
   t=tmin
   
   # simulate detour
   p=runif(1)
   #if p<1/30, add a random unif time between 2 and 4 minutes
   if(p<1/30)
   {
     t=t+runif(1,min=2,max=4)
   }
   
   #cars at the turn onto rt 114, a poison with lambda=3
   
   ncars=rpois(1,3)
   #for each car, add a random normal time
   for(j in ncars)
   {
     t=t+rnorm(1,mean=0.75, sd=0.25)
   }
   
   # add this time to the list
   tsim[i]=t
}


#create a histogram of the distribution under the null of the permutation test

hist(tsim,breaks=20)




```

What does this plot mean?

We have a cluster around 12-14 minutes,  plus a few around 14-17 minutes

The rare detours create a long tail on the right hand side

The standard deviation won't describe this distribution well,  there is a large
right hand tail

We can get a mean

```{R}
mean(tsim)
```

```{R}
max(tsim)
```

```{R}
#get the ecdf

time_cdf=ecdf(tsim)

t=seq(11,20,0.25)
y_cdf=time_cdf(t)
plot(t,y_cdf)
grid()
```

What percentage of the time is my drive 14 minutes or less?

```{R}
time_cdf(14)
```

# Bayesian Linear Regression

Use Bayesian methods to evaluate parameter uncertainty in a regression model

See  https://www.rensvandeschoot.com/tutorials/r-linear-regression-bayesian-using-brms/

for the source material

NOTE: rstan is complex and may not install easily on your system.  If it doesn't
install easily,   don't try to fix matters, just read my discussion of the results


```{r}
library(rstan) 
library(brms)
library(psych) #to get some extended summary statistics
library(tidyverse) # needed for data manipulation and plotting

```

Get the data from a remote site

```{R}
dataPHD <- read.csv2(file="https://raw.githubusercontent.com/LaurentSmeets/Tutorials/master/Blavaan/phd-delays.csv")
colnames(dataPHD) <- c("diff", "child", "sex","age","age2")

```

```{R}
head(dataPHD)
```

diff- the delay time in months between planned completion of a phd and the actual
completion time

child- the number of children the PhD candidate has

sex- sex of the candidate

age- age of candidate

age2- squared age allows non-linear response to age

*Bayes Model 1,  dependence on age and age2 *

this cell will run slowly!

```{R}

model <- brm(formula = diff ~ age + age2, 
             data    = dataPHD,
             seed    = 123)

```

```{R}
summary(model)
```

```{R}
plot(model)

```

We actually get the parameter value, but also a distribution for each parameter,
this is a lot of detail about how certain we are about the model.

We can also specify initial estimates of the parameters and their distributions
called "the priors",   and use the data as a way to update the priors.   This 
is a huge advantage in situations where we know things about the priors.

Here is how we could specify the priors for age and age2

```{R}
priors2 <- c(set_prior("normal(3, 0.632)", class = "b", coef = "age"),
             set_prior("normal(0, 0.316)", class = "b", coef = "age2"))
```

```{R}

model2 <- brm(formula = diff ~ age + age2, 
             data    = dataPHD,
             prior = priors2,
             seed    = 123)

```

The Bayesian caculation is running a MCMC calculation,  at Monte Carlo, Markov Chain
calculation.   It uses a prior distribution of the parameter values and then 
a long series of simulations to update the prior distribution based on the model
and the data, to generate a posterior distribution of parameter values

This trial used a "flat prior" or "uninformative prior" based on the idea that
we have not idea what the parameter values should be.


```{R}
summary(model2)
```

```{R}
plot(model2)
```

If we look at the results from this second Bayesian model, the uncertainty in
the coefficients of age and age2 has been decreased by specifying a more
realistic prior,  incorporating prior knowledge of the parameters.

Bayesian methods can produce more accurate estimates of parameter distributions,
that is the big advantage

# Cross Validation


See

https://rpubs.com/muxicheng/1004550

When we fit a model to data and then try to test the predictive performance of that model
on the same data, we typically find the performance of the model on new
data is not as good.

This is a phenomena called "overfitting",  it's kind of like drawing a curve through
all the points on a graph, the model is too close the data

Ideally we would

1.) Train the model on a "training set"
2.) Go and collect new data as a "test set"
3.) Test our model on the new "test set"

But there are other ways called cross validation to do this

a.) Split the data into "training data" and "validation data"
     fit the model to the training data,  evaluate it on the validation data
b.) Repeated cross validation- systematically split the data many times into
     a training set and a validation set,  train and evaluate- do this many
     times to generate many cross validation measures
     
We will look at only the test/train split method here, using the iris data

Note:require() is an alternate version of library()

```{R}
require(caret)
require(dplyr)
require(tidyverse)

```

Get the iris data

```{R}

data(iris)
str(iris)
```

Split the data into train and test sets

80% is test
20% is train

```{r}
### Data splitting

# set seed to generate a reproducible random sample
set.seed(123)

# create training and testing data set using index, training data contains 80% of the data set
# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.
train.index.vsa <- createDataPartition(iris$Species, p= 0.8, list = FALSE)
train.vsa <- iris[train.index.vsa,]
test.vsa <- iris[-train.index.vsa,]

# see how the the subsets are randomized
role = rep('train',nrow(iris))
role[-train.index.vsa] = 'test'
ggplot(data = cbind(iris,role)) + geom_point(aes(x = Sepal.Length,
                                                 y = Petal.Width,
                                                 color = role))

```
Looking at how the test and train sets split up the data

Now fit a linear regression model to the training data, then use
it to predict the test data 

This computes 

RMSE-   root mean square error, the standard deviation of the error

R^2- the R squared value

MAE- mean average error per point

```{R}
### Training
model.vsa <- lm(Petal.Width ~., data = train.vsa)


### Testing
predictions.vsa <- model.vsa %>% predict(test.vsa)


### Evaluating
test_res=data.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),
           R2 = R2(predictions.vsa, test.vsa$Petal.Width),
           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))

test_res

```

We can do the same thing for the training data

These are called "resubstitution estimates"

```{R}
### Training
model.vsa <- lm(Petal.Width ~., data = train.vsa)


### Testing
predictions.train <- model.vsa %>% predict(train.vsa)


### Evaluating
train_res=data.frame(RMSE = RMSE(predictions.train, train.vsa$Petal.Width),
           R2 = R2(predictions.train, train.vsa$Petal.Width),
           MAE = MAE(predictions.train, train.vsa$Petal.Width))

train_res

```

Notice that the R^2 is just slightly higher for the training data, and the RMSE and MAE are
just slightly lower.

There is not a lot of overfitting here, but there is some

The cross validation estimate are a better estimate of how the model will 
perform on new data